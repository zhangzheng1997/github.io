<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="CAN论文笔记"><meta name="keywords" content=""><meta name="author" content="Zhang"><meta name="copyright" content="Zhang"><title>CAN论文笔记 | Zaneta的小窝</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.2.1"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Model"><span class="toc-number">2.</span> <span class="toc-text">Model</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Embedding-and-LSTM-Layers"><span class="toc-number">2.1.</span> <span class="toc-text">Embedding and LSTM Layers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Task-Specific-Attention-Layer"><span class="toc-number">2.2.</span> <span class="toc-text">Task-Specific Attention Layer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ALSC-attention-Layer"><span class="toc-number">2.2.1.</span> <span class="toc-text">ALSC attention Layer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ACD-Attention-Layer"><span class="toc-number">2.2.2.</span> <span class="toc-text">ACD Attention Layer</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Regularization-Layer"><span class="toc-number">2.3.</span> <span class="toc-text">Regularization Layer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sparse-Regularization"><span class="toc-number">2.3.1.</span> <span class="toc-text">Sparse  Regularization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Orthogonal-Regularization"><span class="toc-number">2.3.2.</span> <span class="toc-text">Orthogonal  Regularization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Task-Specific-Prediction-Layer"><span class="toc-number">2.4.</span> <span class="toc-text">Task-Specific Prediction Layer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ALSC-Prediction"><span class="toc-number">2.4.1.</span> <span class="toc-text">ALSC Prediction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ACD-Prediction"><span class="toc-number">2.4.2.</span> <span class="toc-text">ACD Prediction</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Loss"><span class="toc-number">2.5.</span> <span class="toc-text">Loss</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Experiments"><span class="toc-number">3.</span> <span class="toc-text">Experiments</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Zhang</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">5</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">3</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">2</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Zaneta的小窝</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">CAN论文笔记</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-03-15</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/NLP/">NLP</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>以前的大多数研究都是通过各种基于注意的方法来生成特定于体的句子表征，但是注意力会产生噪声，从而降低性能。因此，本文提出了  constrained  atten-tion  networks  (CAN)，具体来说，一个句子中包含多个方面，我们在多个方面引入正交正则化，在每个方面引入稀疏正则化。</p>
<img src="/Users/zhangzheng/Library/Application Support/typora-user-images/image-20210307100053979.png" alt="image-20210307100053979" style="zoom:50%;" />

<p>如图所示，对于aspect food，我们将模型中的注意力权重可视化（Wang et al，2016），大部分的注意力集中在嘈杂的词“sometimes”和opinion word “ ok”，但ok是有关于service，而不是food。</p>
<p>此外，通过引入ACD作为辅助任务，将CAN扩展到多任务设置，并在ALSC和ACD任务上应用CAN。</p>
<p>对于具有K个aspects的句子S，传统的single-aspect情感分析单独解决每个aspect，需要单独做k次预测，我们同时做K个预测，实现multi-aspect情感分析。</p>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>![image-20210307100030214](/Users/zhangzheng/Library/Application Support/typora-user-images/image-20210307100030214.png)</p>
<p>数据集中有N个预定义的aspect categories：$A = {A_1,…,A_N}$</p>
<p>给定一个句子$S = {w_1,w_2,…,w_L}$，包含K个aspect $A^s = {A^s_1,…,A^s_K}$，$A^s_k \in A$。ALSC任务预测每一个aspect $A^s_k \in A^s$的情感极性。辅助ACD任务检查每个aspect$A_n\in A$是否在句子中被提到了。</p>
<h2 id="Embedding-and-LSTM-Layers"><a href="#Embedding-and-LSTM-Layers" class="headerlink" title="Embedding and LSTM Layers"></a>Embedding and LSTM Layers</h2><p>传统的single-aspect情感分析单独解决每个aspect，这种情况下，具有K个aspect的句子S将被复制K个样本，</p>
<p>例如，一个句子S包含两个aspects：$A^s_1$极性为$p_1$，$A^s_2$极性为$p_2$。两个实例被构建：$&lt;S,A^s_1,p_1&gt;$$&lt;S,A^s_2,p_2&gt;$。本文的multi-aspect情感分析方法将多个方面一起处理，以单一实例$&lt;S,[A^s_1,p_1],[A^s_2,p_2]&gt;$作为输入。</p>
<p>输入的句子${w_1,w_2,…,w_L}$首先转换成向量序列${v_q,v_2,…,v_L}$，句子中K个aspect转变为向量${u^s_1,…,u^s_K}$，aspect categories为${u_1,…,u_N}$。</p>
<p>句子的word embeddings输入到lstm中，输出的隐藏状态$H = {h_1,h_2,…,h_L}$。embedding 和hidden state的size都设为d。</p>
<h2 id="Task-Specific-Attention-Layer"><a href="#Task-Specific-Attention-Layer" class="headerlink" title="Task-Specific Attention Layer"></a>Task-Specific Attention Layer</h2><p>ALSC和ACD任务共享来自LSTM层的隐藏状态，但是他们的注意力权重是单独计算的，注意力权重被用来计算aspect-specific句子表示。</p>
<h3 id="ALSC-attention-Layer"><a href="#ALSC-attention-Layer" class="headerlink" title="ALSC attention Layer"></a>ALSC attention Layer</h3><p>句子S具有K个aspects，$A^S={A^S_1,…,A^S_K}$，对于每一个aspect $A^S_k$，注意力权重计算如下：</p>
<p>​                               $\alpha_k = softmax(z^{aT}tanh(W^{a}_1H+W^a_2(u^s_k\bigotimes e_L)))$</p>
<p>$u^s_k$为aspect $A^s_k$的embedding，$e_L \in \mathbb{}  \mathcal{}^L$是a vector of 1s，$u^s_k\bigotimes e_L$是重复L次拼接$u^s_k$操作，$W^a_1 \in \mathbb{R}^{d\times d}， W^a_2 \in \mathbb{R}^{d\times d} ，z^a \in \mathbb{R}^{d\times d}$是权重矩阵。</p>
<h3 id="ACD-Attention-Layer"><a href="#ACD-Attention-Layer" class="headerlink" title="ACD Attention Layer"></a>ACD Attention Layer</h3><p>本文将ACD任务当作是N个aspect categories的多标签分类任务。对于每一个$A_n\in A$，注意力权重计算如下：</p>
<p>​                                    $\beta_n = softmax(z^{bT}tanh(W^{b}_1H+W^b_2(u^n\bigotimes e_L)))$</p>
<p>，</p>
<p>$u_n$是aspect $A_n$的embedding，$W^b_1 \in \mathbb{R}^{d\times d},W^b_2 \in \mathbb{R}^{d\times d}, z^b \in \mathbb{R}^{d\times d}$为权重矩阵。</p>
<p>ALSC和ACD采用相同的注意力机制，但是不共享参数，这是因为，对于同一个aspect而言，ALSC注意力集中在opinion words上，而ACD注意力集中在aspect target terms上。</p>
<h2 id="Regularization-Layer"><a href="#Regularization-Layer" class="headerlink" title="Regularization Layer"></a>Regularization Layer</h2><p>通过添加约束的注意权重，我们同时处理多个aspects。该层仅用于训练阶段，ground-truth已知，用于计算正则化损失，然后在back propagation中影响参数更新。而在测试阶段，真实的方面是未知的，并且没有计算正则化损失，因此省略了该层。</p>
<h3 id="Sparse-Regularization"><a href="#Sparse-Regularization" class="headerlink" title="Sparse  Regularization"></a>Sparse  Regularization</h3><p>对于每一个aspect，稀疏正则化约束了注意力权重$(\alpha_k)$的分布去集中于更少的单词，例如，$\alpha_k={\alpha_{k1},\alpha_{k2},…,\alpha_{kL}}$。为了使$\alpha_k$稀疏，稀疏正则项定义如下：</p>
<p>​                                      $R_s=|\displaystyle\sum^L_{l=1}\alpha^2_{kl}-1|$，其中$\displaystyle\sum^L_{l=1}\alpha_{kl}=1$且$\alpha_{kl}&gt;0$</p>
<h3 id="Orthogonal-Regularization"><a href="#Orthogonal-Regularization" class="headerlink" title="Orthogonal  Regularization"></a>Orthogonal  Regularization</h3><p>Orthogonal Regularization只应用于不重叠的multi-aspect 句子。句子S包含K个不重叠aspects${A^s_1,…,A^s_K}$，注意力权重向量为${\alpha_1,…,\alpha_K}$，pack them 作为一个二维注意力矩阵$M \in \mathbb{R}^{K\times L}$去计算正交正则项:</p>
<p>$R_o=||M^TM-I||_2$</p>
<p>$I$为单位矩阵。</p>
<p>对于ACD任务，一个句子中有k个aspect提到了，attention vectors为${\beta_1,…,\beta_K}$，有N-K个aspect没有提到，对于N-K个attention vectors求平均，得到$\beta_{avg}$，因此，attention matrix $G={\beta_1,…,\beta_K,\beta_{avg}}$。</p>
<p>由于没有出现的aspect的attention weights通常为一些停用词，这样的做法可以使k个acpect远离这些停用词。</p>
<h2 id="Task-Specific-Prediction-Layer"><a href="#Task-Specific-Prediction-Layer" class="headerlink" title="Task-Specific Prediction Layer"></a>Task-Specific Prediction Layer</h2><p>给定每一个aspect的注意力权重，可以生成aspect specific 句子表示，然后分别对ALSC和ACD任务进行预测。</p>
<h3 id="ALSC-Prediction"><a href="#ALSC-Prediction" class="headerlink" title="ALSC Prediction"></a>ALSC Prediction</h3><p>加权的隐藏状态和最后一个隐藏状态结合生成最后aspect-specific句子表示。</p>
<p>$r^s_k=tanh(W^r_1\bar{h}_k+W^r_2h_L)$</p>
<p>$W^r_1\in \mathbb{R}^{d\times d}W^r_1\in \mathbb{R}^{d\times d}$，$\bar{h}<em>k=\displaystyle \sum^L</em>{l=1}\alpha_{kl}h_l$是aspect k的加权隐藏状态，$r^s_k$被用来预测情感极性。</p>
<p>$\hat{y}_k = softmax(W^a_pr^s_k+b^a_p)$</p>
<p>$W^a_p\in \mathbb{R}^{d\times c}$和$b^a_p\in \mathbb{R}^c$是映射层的参数，c是类别数量。</p>
<p>对于具有K个aspects的句子S，我们同时做K个预测，这就是为什么我们的方法叫做multi-aspect情感分析。</p>
<h3 id="ACD-Prediction"><a href="#ACD-Prediction" class="headerlink" title="ACD Prediction"></a>ACD Prediction</h3><p>我们直接用加权的隐藏状态作为句子表示用于ACD预测。</p>
<p>$r_n=\bar{h}<em>n=\displaystyle \sum^L</em>{l=1}\beta_{nl}h_l$</p>
<p>我们不结合最后一个隐藏状态$h_L$，因为aspect在句子中没有提到。</p>
<p>我们对所有预定义的aspect categories做N次预测：</p>
<p>$\hat{y}_n=sigmoid(W^b_pr_n+b^b_p)$</p>
<p>$W^b_p\in \mathbb{R}^{d\times1}$，$b^b_p$是是一个scalar(标量)。</p>
<h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><p>对于ALSC任务，具有K个aspects的句子S的损失函数：</p>
<p>$L_a= -\displaystyle \sum^K_{k=1} \displaystyle \sum_c y_{kc}log\hat{y_{kc}}$</p>
<p>c是类别数量。</p>
<p>对于ACD任务，每一个预测都是一个二分类任务，N个aspect的句子S的损失函数为：</p>
<p>$L_b=-\displaystyle \sum^N_{n=1}[y_nlog\hat{y_n}+(1-y_n)log(1-\hat{y_n})]$</p>
<p>我们联合训练我们的模特任务然后通过最小化组合损失函数来训练模型中的参数：</p>
<p>$L=L_a+ \frac{1}{N}L_b+\lambda R$</p>
<p>R是正则项，$\lambda$是超参数</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>![image-20210307210832187](/Users/zhangzheng/Library/Application Support/typora-user-images/image-20210307210832187.png)</p>
<p>![image-20210307210849332](/Users/zhangzheng/Library/Application Support/typora-user-images/image-20210307210849332.png)</p>
<p>![image-20210307210902582](/Users/zhangzheng/Library/Application Support/typora-user-images/image-20210307210902582.png)</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Zhang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2021/03/15/NLP/can/">http://yoursite.com/2021/03/15/NLP/can/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/03/16/NLP/tt/"><i class="fa fa-chevron-left">  </i><span>NLP/tt</span></a></div><div class="next-post pull-right"><a href="/2021/03/15/NLP/test-md/"><span>This is a test</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2021 By Zhang</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.7.0"></script><script src="/js/fancybox.js?version=1.7.0"></script><script src="/js/sidebar.js?version=1.7.0"></script><script src="/js/copy.js?version=1.7.0"></script><script src="/js/fireworks.js?version=1.7.0"></script><script src="/js/transition.js?version=1.7.0"></script><script src="/js/scroll.js?version=1.7.0"></script><script src="/js/head.js?version=1.7.0"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>